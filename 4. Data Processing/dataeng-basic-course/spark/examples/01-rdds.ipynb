{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/lucprosa/dataeng-basic-course/blob/main/spark/examples/01-rdds.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_GBE9UsyxwK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BOA_wQSmLd9z"
   },
   "source": [
    "# Data Collections (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d9LeYFsPTjAb"
   },
   "source": [
    "# Setting up PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uYXeODL0T1fO",
    "outputId": "0691bf7a-4779-490d-b28a-e14e6470a724"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "637HFw00T3LP"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master('local').appName('Spark Course').config('spark.ui.port', '4050').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_WW76P6DpLpx"
   },
   "outputs": [],
   "source": [
    "# diff between SparkSession and SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "SqwZjdL9MnL6"
   },
   "outputs": [],
   "source": [
    "sc = SparkSession.getActiveSession().sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGG7R76hMZ3E"
   },
   "source": [
    "# RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "DZaxZW0bMZ3F"
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)\n",
    "\n",
    "### Parallelized collections are created by calling SparkContextâ€™s parallelize method on an existing iterable or collection in your driver program\n",
    "### The elements of the collection are copied to form a distributed dataset that can be operated on in parallel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "a8eCChgrMZ3H"
   },
   "outputs": [],
   "source": [
    "### Run parallel operations\n",
    "distData.reduce(lambda a, b: a + b)\n",
    "\n",
    "### One important parameter for parallel collections is the number of partitions to cut the dataset into\n",
    "### Spark will run one task for each partition of the cluster\n",
    "### Typically you want 2-4 partitions for each CPU in your cluster\n",
    "### Normally, Spark tries to set the number of partitions automatically based on your cluster\n",
    "### However, you can also set it manually\n",
    "\n",
    "distData = sc.parallelize(data, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8UzHNVzynNm-"
   },
   "outputs": [],
   "source": [
    "#!mkdir /content/files/\n",
    "text = \"red yellow white green\"\n",
    "\n",
    "text_file = open(\"/content/files/data.txt\", \"w\")\n",
    "text_file.write(text)\n",
    "text_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5qs9KNmJMZ3I",
    "outputId": "c84a2043-2f6e-403e-811c-9ab360f9be78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data.txt MapPartitionsRDD[1] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read from external datasets\n",
    "distFile = sc.textFile(\"data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1ftjIsEMZ3J"
   },
   "outputs": [],
   "source": [
    "### RDDs support two types of operations: transformations, which create a new dataset from an existing one, and actions, which return a value to the driver program after running a computation on the dataset\n",
    "\n",
    "### Example:\n",
    "### map is a transformation that passes each dataset element through a function and returns a new RDD representing the results\n",
    "### reduce is an action that aggregates all the elements of the RDD using some function and returns the final result to the driver program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PJr1dWvFMZ3J",
    "outputId": "ca55ad9a-ded3-4aa1-adf7-d9890a1fe3bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "### All transformations in Spark are lazy\n",
    "### The transformations are only computed when an action requires a result to be returned to the driver program\n",
    "\n",
    "lines = sc.textFile(\"/content/files/data.txt\")\n",
    "lineLengths = lines.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)\n",
    "print(totalLength)\n",
    "\n",
    "### The first line defines a base RDD from an external file.\n",
    "### lines is merely a pointer to the file\n",
    "### The second line defines lineLengths as the result of a map transformation\n",
    "### lineLengths is not immediately computed, due to laziness\n",
    "### reducer is an actions so Spark breaks the computation into tasks to run on separate machines\n",
    "### and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QKflP9NxpweG",
    "outputId": "f460adf9-aa73-47c5-c9ca-200be85b0492"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['red yellow white green']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eFgcn2llperu",
    "outputId": "5843a067-3cd8-4b3a-836f-3b1cfd8243b1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[12] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### if we want to use lineLenghts again\n",
    "lineLengths.persist()\n",
    "### before the reduce, which would cause lineLengths to be saved in memory after the first time it is computed."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
