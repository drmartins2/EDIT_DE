{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drmartins2/EDIT_DE/blob/main/spark/challenges/challenge_1_drm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOA_wQSmLd9z"
      },
      "source": [
        "# CHALLENGE 1\n",
        "##  Implement INGESTION process\n",
        "- Set up path in the \"lake\"\n",
        "  - !mkdir -p /content/lake/bronze\n",
        "\n",
        "- Read data from API https://api.carrismetropolitana.pt/\n",
        "  - Endpoints:\n",
        "    - vehicles\n",
        "    - lines\n",
        "    - municipalities\n",
        "  - Use StructFields to enforce schema\n",
        "\n",
        "- Transformations\n",
        "  - vehicles\n",
        "    - create \"date\" extracted from \"timestamp\" column (format: date - yyyy-mm-dd or yyyymmdd)\n",
        "\n",
        "- Write data as PARQUET into the BRONZE layer (/content/lake/bronze)\n",
        "  - Partition \"vehicles\" by \"date\" column\n",
        "  - Paths:\n",
        "    - vehicles - path: /content/lake/bronze/vehicles\n",
        "    - lines - path: /content/lake/bronze/lines\n",
        "    - municipalities - path: /content/lake/bronze/municipalities\n",
        "  - Make sure there is only 1 single parquet created\n",
        "  - Use overwrite as write mode"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "2b84ceac-0fbf-4892-8b98-2795a4a40f31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.master('local').appName('Challenge 1').config('spark.ui.port', '4050').getOrCreate()"
      ],
      "metadata": {
        "id": "kyWynLq3IgWL"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/lake/bronze\n",
        "print('Bronze layer created')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vVKKlWYi4PO5",
        "outputId": "24968494-15e8-416a-ee22-963c15bc61f2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bronze layer created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define ETLFlow Class"
      ],
      "metadata": {
        "id": "Yyp5_1VfRt63"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import DataFrame\n",
        "from pyspark.sql.types import *\n",
        "import requests\n",
        "\n",
        "class ETLFlow:\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        self.spark = spark\n",
        "\n",
        "    # Extract data from API endpoint\n",
        "    # Parameters:\n",
        "    #   url: The API endpoint URL.\n",
        "    #   schema: Optional schema to enforce on the JSON data\n",
        "    # Returns:\n",
        "    #   DataFrame containing the extracted data.\n",
        "    def ReadAPI(self, url: str, schema: StructType = None) -> DataFrame:\n",
        "        response = requests.get(url)  # Fetch data from the API\n",
        "        rdd = self.spark.sparkContext.parallelize(response.json())  # Convert JSON response to RDD\n",
        "        if schema:\n",
        "            df = self.spark.read.schema(schema).json(rdd)  # Read RDD into DataFrame with schema\n",
        "        else:\n",
        "            df = self.spark.read.json(rdd)  # Read RDD into DataFrame without schema\n",
        "        return df\n",
        "\n",
        "\n",
        "    # Load a DataFrame into storage in the specified format.\n",
        "    # Parameters:\n",
        "    #   df: The DataFrame to be saved.\n",
        "    #   format: The format to save the DataFrame in (e.g., parquet).\n",
        "    #   path: The path where the DataFrame should be saved.\n",
        "    #   partition_column: Optional column to partition the data by.\n",
        "    def load(self, df: DataFrame, format: str, path: str, partition_column: str = None) -> None:\n",
        "        if partition_column:\n",
        "            # Save as a single file, partitioned by the specified column\n",
        "            df.coalesce(1).write.mode(\"overwrite\").partitionBy(partition_column).format(format).save(path)\n",
        "        else:\n",
        "            # Save as a single file without partitioning\n",
        "            df.coalesce(1).write.mode(\"overwrite\").format(format).save(path)\n",
        "\n"
      ],
      "metadata": {
        "id": "fLPc2NcsKplX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aux get endpoint schema"
      ],
      "metadata": {
        "id": "091iee9TsTCz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ReadAPI(url: str, schema: StructType = None) -> DataFrame:\n",
        "    response = requests.get(url)  # Fetch data from the API\n",
        "    rdd = spark.sparkContext.parallelize(response.json())  # Convert JSON response to RDD\n",
        "    if schema:\n",
        "        df = spark.read.schema(schema).json(rdd)  # Read RDD into DataFrame with schema\n",
        "    else:\n",
        "        df = spark.read.json(rdd)  # Read RDD into DataFrame without schema\n",
        "    return df\n",
        "\n",
        "# Print the schema for a given endpoint\n",
        "# Parameters:\n",
        "#   df: The DataFrame whose schema is to be printed\n",
        "#   endpoint: The name of the API endpoint\n",
        "def print_schema(df: DataFrame, endpoint: str) -> None: # Removed self from the arguments\n",
        "    print(f\"\\nSchema for {endpoint} endpoint:\")\n",
        "    df.printSchema()\n",
        "\n",
        "# Vehicles\n",
        "df = ReadAPI(url=\"https://api.carrismetropolitana.pt/vehicles\", schema=None)\n",
        "print_schema(df, \"vehicles\")\n",
        "\n",
        "# Municipalities\n",
        "df = ReadAPI(url=\"https://api.carrismetropolitana.pt/municipalities\", schema=None)\n",
        "print_schema(df, \"municipalities\")\n",
        "\n",
        "# Lines\n",
        "df = ReadAPI(url=\"https://api.carrismetropolitana.pt/lines\", schema=None)\n",
        "print_schema(df, \"lines\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z3hYmrlbkQjn",
        "outputId": "09b708a0-16ad-4adf-9171-c8448161dec9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Schema for vehicles endpoint:\n",
            "root\n",
            " |-- bearing: long (nullable = true)\n",
            " |-- block_id: string (nullable = true)\n",
            " |-- current_status: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- lat: double (nullable = true)\n",
            " |-- line_id: string (nullable = true)\n",
            " |-- lon: double (nullable = true)\n",
            " |-- pattern_id: string (nullable = true)\n",
            " |-- route_id: string (nullable = true)\n",
            " |-- schedule_relationship: string (nullable = true)\n",
            " |-- shift_id: string (nullable = true)\n",
            " |-- speed: double (nullable = true)\n",
            " |-- stop_id: string (nullable = true)\n",
            " |-- timestamp: long (nullable = true)\n",
            " |-- trip_id: string (nullable = true)\n",
            "\n",
            "\n",
            "Schema for municipalities endpoint:\n",
            "root\n",
            " |-- district_id: string (nullable = true)\n",
            " |-- district_name: string (nullable = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- name: string (nullable = true)\n",
            " |-- prefix: string (nullable = true)\n",
            " |-- region_id: string (nullable = true)\n",
            " |-- region_name: string (nullable = true)\n",
            "\n",
            "\n",
            "Schema for lines endpoint:\n",
            "root\n",
            " |-- _corrupt_record: string (nullable = true)\n",
            " |-- color: string (nullable = true)\n",
            " |-- facilities: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- id: string (nullable = true)\n",
            " |-- localities: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- long_name: string (nullable = true)\n",
            " |-- municipalities: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- patterns: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- routes: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- short_name: string (nullable = true)\n",
            " |-- text_color: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define ETLTask Class"
      ],
      "metadata": {
        "id": "K1eLN4qdRk-A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "class ETLTask(ETLFlow):\n",
        "    def __init__(self, spark: SparkSession) -> None:\n",
        "        # Initialize ETLTask by inheriting from ETLFlow\n",
        "        super().__init__(spark)\n",
        "\n",
        "\n",
        "    # Ingest vehicle data from the API and load it into the bronze layer.\n",
        "    #   Extract data from 'vehicles' endpoint.\n",
        "    #   Add 'date' column derived from 'timestamp'.\n",
        "    #   Saves data as parquet, partitioned by 'date'.\n",
        "    def ingestion_vehicles(self):\n",
        "        # Define schema for vehicle\n",
        "        vehicle_schema = StructType([\n",
        "            StructField('bearing', IntegerType(), True),\n",
        "            StructField('block_id', StringType(), True),\n",
        "            StructField('current_status', StringType(), True),\n",
        "            StructField('id', StringType(), True),\n",
        "            StructField('lat', FloatType(), True),\n",
        "            StructField('line_id', StringType(), True),\n",
        "            StructField('lon', FloatType(), True),\n",
        "            StructField('pattern_id', StringType(), True),\n",
        "            StructField('route_id', StringType(), True),\n",
        "            StructField('schedule_relationship', StringType(), True),\n",
        "            StructField('shift_id', StringType(), True),\n",
        "            StructField('speed', FloatType(), True),\n",
        "            StructField('stop_id', StringType(), True),\n",
        "            StructField('timestamp', TimestampType(), True),\n",
        "            StructField('trip_id', StringType(), True)\n",
        "        ])\n",
        "\n",
        "        # Extract data using defined schema\n",
        "        df = self.ReadAPI(url=\"https://api.carrismetropolitana.pt/vehicles\", schema=vehicle_schema)\n",
        "\n",
        "        # Create \"date\" column from \"timestamp\"\n",
        "        df = df.withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
        "        df.show()\n",
        "\n",
        "        # Load data into the bronze layer, partitioned by \"date\"\n",
        "        self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/vehicles\", partition_column=\"date\")\n",
        "\n",
        "\n",
        "    # Ingest line data from the API and load it into the bronze layer.\n",
        "    #   Extracts data from 'lines' endpoint.\n",
        "    #   Saves data as parquet without partitioning.\n",
        "    def ingestion_lines(self):\n",
        "        # Define schema for lines\n",
        "        lines_schema = StructType([\n",
        "            StructField('color', StringType(), True),\n",
        "            StructField('facilities', ArrayType(StringType()), True),\n",
        "            StructField('id', StringType(), True),\n",
        "            StructField('localities', ArrayType(StringType()), True),\n",
        "            StructField('long_name', StringType(), True),\n",
        "            StructField('municipalities', ArrayType(StringType()), True),\n",
        "            StructField('patterns', ArrayType(StringType()), True),\n",
        "            StructField('routes', ArrayType(StringType()), True),\n",
        "            StructField('short_name', StringType(), True),\n",
        "            StructField('text_color', StringType(), True)\n",
        "        ])\n",
        "\n",
        "        # Extract data using defined schema\n",
        "        df = self.ReadAPI(url=\"https://api.carrismetropolitana.pt/lines\", schema=lines_schema)\n",
        "        df.show()\n",
        "\n",
        "        # Load data into the bronze layer\n",
        "        self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/lines\")\n",
        "\n",
        "\n",
        "    # Ingest municipality data from the API and load it into the bronze layer.\n",
        "    #   Extracts data from 'municipalities' endpoint.\n",
        "    #   Saves data as parquet without partitioning.\n",
        "    def ingestion_municipalities(self):\n",
        "       # Define schema for municipalities\n",
        "       municipalities_schema = StructType([\n",
        "           StructField('district_name', StringType(), True),\n",
        "           StructField('id', StringType(), True),\n",
        "           StructField('name', StringType(), True),\n",
        "           StructField('prefix', StringType(), True),\n",
        "           StructField('region_id', StringType(), True),\n",
        "           StructField('region_name', StringType(), True)\n",
        "       ])\n",
        "\n",
        "\n",
        "       # Extract data using defined schema\n",
        "       df = self.ReadAPI(url=\"https://api.carrismetropolitana.pt/municipalities\", schema=municipalities_schema)\n",
        "       df.show()\n",
        "\n",
        "       # Load data into the bronze layer\n",
        "       self.load(df=df, format=\"parquet\", path=\"/content/lake/bronze/municipalities\")\n",
        "\n",
        "print('ETLTask class defined for specific API endpoint ingestion')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtT72tGqQDnl",
        "outputId": "e4c23401-005d-4433-c0df-51997b503745"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ETLTask class defined for specific API endpoint ingestion\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Execute ETL Process"
      ],
      "metadata": {
        "id": "ruOs_yhLQNfX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.master('local').appName('ETL Program').getOrCreate()\n",
        "\n",
        "print(\"Starting ETL program\")\n",
        "etl = ETLTask(spark)\n",
        "\n",
        "# Run tasks for each API endpoint\n",
        "\n",
        "# Ingest vehicle data\n",
        "print(\"Running Task - Ingestion Vehicles\")\n",
        "etl.ingestion_vehicles()\n",
        "\n",
        "# Ingest line data\n",
        "print(\"Running Task - Ingestion Lines\")\n",
        "etl.ingestion_lines()\n",
        "\n",
        "# Ingest municipality data\n",
        "print(\"Running Task - Ingestion Municipalities\")\n",
        "etl.ingestion_municipalities()\n",
        "\n",
        "print(\"ETL program completed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOMDOpKjQJXP",
        "outputId": "27ac6cf9-6b8f-44f3-9cd3-98086d5f5f60"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ETL program\n",
            "Running Task - Ingestion Vehicles\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "|bearing|            block_id|current_status|      id|      lat|line_id|      lon|pattern_id|route_id|schedule_relationship|    shift_id|    speed|stop_id|          timestamp|             trip_id|      date|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "|    229|20241126-64010212...| IN_TRANSIT_TO|44|12607| 38.53006|   4441|-8.883835|  4441_0_1|  4441_0|            SCHEDULED|112380234560|      7.5| 160403|2024-11-26 15:59:21|4441_0_1|2600|153...|2024-11-26|\n",
            "|    179|20241126-64010065...| IN_TRANSIT_TO|44|12551| 38.64447|   4602|-9.036845|  4602_0_1|  4602_0|            SCHEDULED|121780234560|5.2777777| 090227|2024-11-26 15:59:25|4602_0_1|2600|155...|2024-11-26|\n",
            "|    157|20241126-64010591...| IN_TRANSIT_TO|44|12743| 38.67385|   4730|-9.173968|  4730_0_1|  4730_0|            SCHEDULED|113240234560|      0.0| 140139|2024-11-26 15:59:31|4730_0_1|2600|153...|2024-11-26|\n",
            "|     28|20241126-64010172...| IN_TRANSIT_TO|44|12612| 38.53013|   4906|-8.885982|  4906_0_1|  4906_0|            SCHEDULED|113110234560|      0.0| 160031|2024-11-26 15:59:34|4906_0_1|2600|160...|2024-11-26|\n",
            "|     67|20241126-64010122...| IN_TRANSIT_TO|44|12082| 38.67956|   4102| -8.99032|  4102_0_1|  4102_0|            SCHEDULED|121240234560|13.333333| 090083|2024-11-26 15:59:21|4102_0_1|2600|154...|2024-11-26|\n",
            "|    164|20241126-64010087...| IN_TRANSIT_TO|44|12527|38.700336|   4504| -9.00548|  4504_0_1|  4504_0|            SCHEDULED|121580230560|      0.0| 100138|2024-11-26 15:59:21|4504_0_1|2600|160...|2024-11-26|\n",
            "|     39|20241126-64010025...| IN_TRANSIT_TO|44|12696|38.674072|   4701|-8.971512|  4701_0_2|  4701_0|            SCHEDULED|123230234560|11.944445| 100190|2024-11-26 15:59:30|4701_0_2|2600|153...|2024-11-26|\n",
            "|    166|             1532-11|    STOPPED_AT| 42|2206|38.794083|   2524|-9.173533|  2524_0_3|  2524_0|            SCHEDULED|        1556|      7.5| 110391|2024-11-26 15:58:49|2524_0_3|1|1|1515...|2024-11-26|\n",
            "|    142|             1072-11| IN_TRANSIT_TO| 42|2363|38.803635|   2711| -9.12319|  2711_0_1|  2711_0|            SCHEDULED|        1043| 8.888889| 077719|2024-11-26 15:59:25|2711_0_1|1|1|1540...|2024-11-26|\n",
            "|    270|20241126-64010123...|   INCOMING_AT|44|12503|38.706978|   4504| -8.94162|  4504_0_2|  4504_0|            SCHEDULED|121230230560|16.944445| 100336|2024-11-26 15:59:31|4504_0_2|2600|153...|2024-11-26|\n",
            "|    150|20241126-64010029...| IN_TRANSIT_TO|44|12716|38.768707|   4702|-9.104575|  4702_0_2|  4702_0|            SCHEDULED|123390234560|      0.0| 060009|2024-11-26 15:59:21|4702_0_2|2600|151...|2024-11-26|\n",
            "|     32|20241126-64010292...|   INCOMING_AT|44|12089|38.525265|   4412|-8.862213|  4412_0_1|  4412_0|            SCHEDULED|111310234560|     12.5| 160209|2024-11-26 15:59:32|4412_0_1|2600|154...|2024-11-26|\n",
            "|    181|20241126-64010084...| IN_TRANSIT_TO|44|12518|38.719982|   4512|-8.972934|  4512_0_1|  4512_0|            SCHEDULED|121610234560|15.833333| 100012|2024-11-26 15:59:21|4512_0_1|2600|154...|2024-11-26|\n",
            "|    336|20241126-64010264...| IN_TRANSIT_TO|44|12569| 38.52144|   4641|-9.015513|  4641_0_2|  4641_0|            SCHEDULED|111380234560|1.9444444| 160788|2024-11-26 15:59:23|4641_0_2|2600|153...|2024-11-26|\n",
            "|     81|20241126-64010227...| IN_TRANSIT_TO|44|12086| 38.52297|   4427| -8.89505|  4427_0_1|  4427_0|            SCHEDULED|112240234560|      0.0| 160161|2024-11-26 15:59:21|4427_0_1|2600|160...|2024-11-26|\n",
            "|    278|20241126-64010572...| IN_TRANSIT_TO|44|12672|  38.7032|   4706|-8.964028|  4706_0_2|  4706_0|            SCHEDULED|123380234560|      0.0| 100009|2024-11-26 15:59:29|4706_0_2|2600|153...|2024-11-26|\n",
            "|    321|           1_1466-11| IN_TRANSIT_TO| 41|1718|38.706436|   1113|-9.232562|  1113_0_3|  1113_0|            SCHEDULED|        1473|      0.0| 120212|2024-11-26 15:59:23|1113_0_3_1530_155...|2024-11-26|\n",
            "|    309|           1_1704-11| IN_TRANSIT_TO| 41|1207| 38.81442|   1249|-9.368125|  1249_0_1|  1249_0|            SCHEDULED|        1782|6.9444447| 171673|2024-11-26 15:59:20|1249_0_1_1530_155...|2024-11-26|\n",
            "|     47|           1_1423-11| IN_TRANSIT_TO| 41|1351|38.714863|   1103|-9.241857|  1103_0_1|  1103_0|            SCHEDULED|        1498|10.555555| 120173|2024-11-26 15:59:13|1103_0_1_1530_155...|2024-11-26|\n",
            "|      0|           1_1056-11| IN_TRANSIT_TO| 41|1877|38.749844|   1510|-9.247145|  1510_0_1|  1510_0|            SCHEDULED|        1081| 8.055555| 172055|2024-11-26 15:58:56|1510_0_1_1530_155...|2024-11-26|\n",
            "+-------+--------------------+--------------+--------+---------+-------+---------+----------+--------+---------------------+------------+---------+-------+-------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Ingestion Lines\n",
            "+-------+----------+----+--------------------+--------------------+--------------+--------------------+--------------------+----------+----------+\n",
            "|  color|facilities|  id|          localities|           long_name|municipalities|            patterns|              routes|short_name|text_color|\n",
            "+-------+----------+----+--------------------+--------------------+--------------+--------------------+--------------------+----------+----------+\n",
            "|#C61D23|        []|1001|[Alfragide, Amado...|Alfragide (Estr S...|        [1115]|[1001_0_1, 1001_0_2]|            [1001_0]|      1001|   #FFFFFF|\n",
            "|#C61D23|        []|1002|[Reboleira, Amado...|Reboleira (Estaçã...|        [1115]|          [1002_0_3]|            [1002_0]|      1002|   #FFFFFF|\n",
            "|#C61D23|        []|1003|[Amadora, Amadora...|Amadora (Estação ...|        [1115]|[1003_0_1, 1003_0_2]|            [1003_0]|      1003|   #FFFFFF|\n",
            "|#C61D23|        []|1004|[Amadora, Moinhos...|Amadora (Estação ...|        [1115]|          [1004_0_3]|            [1004_0]|      1004|   #FFFFFF|\n",
            "|#C61D23|        []|1005|[Amadora, Casal d...|Amadora (Estação ...|        [1115]|[1005_0_1, 1005_0...|[1005_0, 1005_1, ...|      1005|   #FFFFFF|\n",
            "|#C61D23|        []|1006|[Amadora, Moinhos...|Amadora (Estação ...|        [1115]|[1006_0_1, 1006_0...|    [1006_0, 1006_1]|      1006|   #FFFFFF|\n",
            "|#3D85C6|        []|1008|                NULL|Amadora Este (Met...|        [1115]|          [1008_0_3]|            [1008_0]|      1008|   #FFFFFF|\n",
            "|#3D85C6|        []|1009|   [Amadora, Sintra]|Amadora (Hospital...|  [1115, 1111]|          [1009_0_3]|            [1009_0]|      1009|   #FFFFFF|\n",
            "|#C61D23|        []|1010|[Brandoa, Amadora...|Brandoa (Pólo Esc...|        [1115]|[1010_0_1, 1010_0_2]|            [1010_0]|      1010|   #FFFFFF|\n",
            "|#C61D23|        []|1011|[Brandoa, Amadora...|Brandoa (Largo) -...|        [1115]|[1011_0_1, 1011_0...|    [1011_0, 1011_1]|      1011|   #FFFFFF|\n",
            "|#3D85C6|        []|1012|[Amadora, Brandoa...|Alfornelos Metro ...|        [1115]|          [1012_0_3]|            [1012_0]|      1012|   #FFFFFF|\n",
            "|#C61D23|        []|1013|  [Amadora, Atalaia]|Amadora (Cemitéri...|        [1115]|[1013_0_1, 1013_0_2]|            [1013_0]|      1013|   #FFFFFF|\n",
            "|#C61D23|        []|1014|[Amadora, Rebolei...|Amadora (Cemitéri...|        [1115]|[1014_0_1, 1014_0_2]|            [1014_0]|      1014|   #FFFFFF|\n",
            "|#3D85C6|        []|1015|[Reboleira, Amado...|Reboleira (Estaçã...|        [1115]|          [1015_0_3]|            [1015_0]|      1015|   #FFFFFF|\n",
            "|#C61D23|        []|1101|[Alfragide, Oeira...|Alfragide (Centro...|        [1110]|[1101_0_1, 1101_0...|    [1101_0, 1101_1]|      1101|   #FFFFFF|\n",
            "|#C61D23|        []|1103|[Algés, Oeiras, Q...|Algés (Estação) -...|        [1110]|[1103_0_1, 1103_0_2]|            [1103_0]|      1103|   #FFFFFF|\n",
            "|#C61D23|        []|1105|[Algés, Oeiras, M...|Algés (Estação) -...|        [1110]|[1105_0_1, 1105_0...|    [1105_0, 1105_1]|      1105|   #FFFFFF|\n",
            "|#C61D23|        []|1106|[Queluz Baixo, Oe...|Queluz Baixo (Cen...|        [1110]|[1106_0_2, 1106_1...|    [1106_0, 1106_1]|      1106|   #FFFFFF|\n",
            "|#C61D23|        []|1107|[Algés, Oeiras, Q...|Algés (Estação) -...|        [1110]|[1107_0_1, 1107_0_2]|            [1107_0]|      1107|   #FFFFFF|\n",
            "|#3D85C6|        []|1109|[Carnaxide, Oeira...|Carnaxide via Out...|        [1110]|          [1109_0_3]|            [1109_0]|      1109|   #FFFFFF|\n",
            "+-------+----------+----+--------------------+--------------------+--------------+--------------------+--------------------+----------+----------+\n",
            "only showing top 20 rows\n",
            "\n",
            "Running Task - Ingestion Municipalities\n",
            "+-------------+----+--------------------+------+---------+----------------+\n",
            "|district_name|  id|                name|prefix|region_id|     region_name|\n",
            "+-------------+----+--------------------+------+---------+----------------+\n",
            "|        Évora|0712|        Vendas Novas|    19|    PT187|Alentejo Central|\n",
            "|       Lisboa|1101|            Alenquer|    20|    PT16B|           Oeste|\n",
            "|       Lisboa|1102|   Arruda dos Vinhos|    20|    PT16B|           Oeste|\n",
            "|       Lisboa|1105|             Cascais|    05|    PT170|             AML|\n",
            "|       Lisboa|1106|              Lisboa|    06|    PT170|             AML|\n",
            "|       Lisboa|1107|              Loures|    07|    PT170|             AML|\n",
            "|       Lisboa|1109|               Mafra|    08|    PT170|             AML|\n",
            "|       Lisboa|1110|              Oeiras|    12|    PT170|             AML|\n",
            "|       Lisboa|1111|              Sintra|    17|    PT170|             AML|\n",
            "|       Lisboa|1112|Sobral de Monte A...|    20|    PT16B|           Oeste|\n",
            "|       Lisboa|1113|       Torres Vedras|    20|    PT16B|           Oeste|\n",
            "|       Lisboa|1114| Vila Franca de Xira|    18|    PT170|             AML|\n",
            "|       Lisboa|1115|             Amadora|    03|    PT170|             AML|\n",
            "|       Lisboa|1116|            Odivelas|    11|    PT170|             AML|\n",
            "|      Setúbal|1502|           Alcochete|    01|    PT170|             AML|\n",
            "|      Setúbal|1503|              Almada|    02|    PT170|             AML|\n",
            "|      Setúbal|1504|            Barreiro|    04|    PT170|             AML|\n",
            "|      Setúbal|1506|               Moita|    09|    PT170|             AML|\n",
            "|      Setúbal|1507|             Montijo|    10|    PT170|             AML|\n",
            "|      Setúbal|1508|             Palmela|    13|    PT170|             AML|\n",
            "+-------------+----+--------------------+------+---------+----------------+\n",
            "only showing top 20 rows\n",
            "\n",
            "ETL program completed\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}