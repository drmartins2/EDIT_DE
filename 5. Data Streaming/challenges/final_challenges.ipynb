{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/drmartins2/EDIT_DE/blob/main/5.%20Data%20Streaming/challenges/final_challenges.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_GBE9UsyxwK"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9LeYFsPTjAb"
      },
      "source": [
        "# Setting up PySpark"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYXeODL0T1fO",
        "outputId": "23bf58fb-1186-4ba1-f109-73050abad15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.10/dist-packages (3.5.3)\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n"
          ]
        }
      ],
      "source": [
        "%pip install pyspark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rcybt71kTDNt"
      },
      "source": [
        "# Context\n",
        "Message events are coming from platform message broker (kafka, pubsub, kinesis...).\n",
        "You need to process the data according to the requirements.\n",
        "\n",
        "Message schema:\n",
        "- timestamp\n",
        "- value\n",
        "- event_type\n",
        "- message_id\n",
        "- country_id\n",
        "- user_id\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyPORKNSYvV"
      },
      "source": [
        "# Challenge 1\n",
        "\n",
        "Step 1\n",
        "- Change exising producer\n",
        "\t- Change parquet location to \"/content/lake/bronze/messages/data\"\n",
        "\t- Add checkpoint (/content/lake/bronze/messages/checkpoint)\n",
        "\t- Delete /content/lake/bronze/messages and reprocess data\n",
        "\t- For reprocessing, run the streaming for at least 1 minute, then stop it\n",
        "\n",
        "Step 2\n",
        "- Implement new stream job to read from messages in bronze layer and split result in two locations\n",
        "\t- \"messages_corrupted\"\n",
        "\t\t- logic: event_status is null, empty or equal to \"NONE\"\n",
        "    - extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages_corrupted/data\n",
        "\n",
        "\t- \"messages\"\n",
        "\t\t- logic: not corrupted data\n",
        "\t\t- extra logic: add country name by joining message with countries dataset\n",
        "\t\t- partition by \"date\" -extract it from timestamp\n",
        "\t\t- location: /content/lake/silver/messages/data\n",
        "\n",
        "\t- technical requirements\n",
        "\t\t- add checkpint (choose location)\n",
        "\t\t- use StructSchema\n",
        "\t\t- Set trigger interval to 5 seconds\n",
        "\t\t- run streaming for at least 20 seconds, then stop it\n",
        "\n",
        "\t- alternatives\n",
        "\t\t- implementing single streaming job with foreach/- foreachBatch logic to write into two locations\n",
        "\t\t- implementing two streaming jobs, one for messages and another for messages_corrupted\n",
        "\t\t- (paying attention on the paths and checkpoints)\n",
        "\n",
        "\n",
        "  - Check results:\n",
        "    - results from messages in bronze layer should match with the sum of messages+messages_corrupted in the silver layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Udk3tohSaXOH",
        "outputId": "4d833fa4-b01d-4eee-eaef-1319862d474c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting faker\n",
            "  Downloading Faker-33.1.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.4 in /usr/local/lib/python3.10/dist-packages (from faker) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from faker) (4.12.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.4->faker) (1.17.0)\n",
            "Downloading Faker-33.1.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.9 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.6/1.9 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: faker\n",
            "Successfully installed faker-33.1.0\n"
          ]
        }
      ],
      "source": [
        "%pip install faker"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cDGMKwBdi1qy"
      },
      "source": [
        "# Producer [DEPRECATED]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPCOdivrfhYh",
        "outputId": "b13cd964-28de-4db8-b495-066e1b2c456d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "  fake = Faker()\n",
        "  new_columns = {\n",
        "      'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "      'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "      'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "      'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "      'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "  }\n",
        "  df = df.withColumns(new_columns)\n",
        "  return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "  enrich = enrich_data(df)\n",
        "  enrich.write.mode(\"append\").format(\"parquet\").save(\"content/lake/bronze/messages\")\n",
        "\n",
        "# read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# write stream\n",
        "query = (df_stream.writeStream\n",
        ".outputMode('append')\n",
        ".trigger(processingTime='1 seconds')\n",
        ".foreachBatch(insert_messages)\n",
        ".start()\n",
        ")\n",
        "\n",
        "query.awaitTermination(60)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "KNyUK3yplDhg"
      },
      "outputs": [],
      "source": [
        "query.stop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZWQExsnzlMFe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b36e63-217d-4bda-e9a7-f58730e0f7f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-16 12:01:...|    0|   CLICKED|2d60aaa5-f9b9-45a...|   CHAT|      2004|   1040|\n",
            "|2024-12-16 12:02:...|    2|   CLICKED|2d60aaa5-f9b9-45a...|   CHAT|      2004|   1040|\n",
            "|2024-12-16 12:02:...|    4|   CLICKED|2d60aaa5-f9b9-45a...|   CHAT|      2004|   1040|\n",
            "|2024-12-16 12:01:...|    1|   CLICKED|2d60aaa5-f9b9-45a...|   CHAT|      2004|   1040|\n",
            "|2024-12-16 12:02:...|    3|   CLICKED|2d60aaa5-f9b9-45a...|   CHAT|      2004|   1040|\n",
            "|2024-12-16 12:11:...|  542|  RECEIVED|f8b87f1e-e2ac-454...|  EMAIL|      2011|   1022|\n",
            "|2024-12-16 12:04:...|  172|  RECEIVED|a8c4c974-3fa1-4e4...|  EMAIL|      2004|   1049|\n",
            "|2024-12-16 12:12:...|  606|  RECEIVED|67633b06-5b9e-459...|  EMAIL|      2003|   1003|\n",
            "|2024-12-16 12:03:...|   65|  RECEIVED|67633b06-5b9e-459...|  EMAIL|      2010|   1005|\n",
            "|2024-12-16 12:13:...|  688|  RECEIVED|42b1dc78-09c1-411...|  EMAIL|      2014|   1005|\n",
            "|2024-12-16 12:02:...|   25|  RECEIVED|962151bb-5f24-487...|  EMAIL|      2000|   1050|\n",
            "|2024-12-16 12:05:...|  229|  RECEIVED|cc2f8908-b3b0-4a2...|  EMAIL|      2009|   1036|\n",
            "|2024-12-16 12:02:...|   15|  RECEIVED|97d0934b-0931-4c7...|  EMAIL|      2004|   1003|\n",
            "|2024-12-16 12:16:...|  854|  RECEIVED|16de1fba-78af-4c9...|  EMAIL|      2002|   1012|\n",
            "|2024-12-16 12:13:...|  700|  RECEIVED|f2bc4bc1-58de-495...|  EMAIL|      2007|   1020|\n",
            "|2024-12-16 12:13:...|  709|  RECEIVED|83fe2d98-8bf8-4ee...|  EMAIL|      2004|   1019|\n",
            "|2024-12-16 12:11:...|  589|  RECEIVED|b99fbd6f-896f-47a...|  EMAIL|      2014|   1045|\n",
            "|2024-12-16 12:08:...|  412|  RECEIVED|ee3fb4ec-245b-410...|  EMAIL|      2003|   1001|\n",
            "|2024-12-16 12:15:...|  822|  RECEIVED|256464ce-bf8d-495...|  EMAIL|      2003|   1042|\n",
            "|2024-12-16 12:13:...|  692|  RECEIVED|7c39123f-71bd-412...|  EMAIL|      2012|   1025|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = spark.read.format(\"parquet\").load(\"content/lake/bronze/messages/*\")\n",
        "df.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraxHCycMdEZ"
      },
      "source": [
        "# Additional datasets [DEPRECATED]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cfsus3dxMcQI"
      },
      "outputs": [],
      "source": [
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#New Producer"
      ],
      "metadata": {
        "id": "vukwHaBZgdFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql import DataFrame\n",
        "from faker import Faker\n",
        "from pyspark.sql import SparkSession\n",
        "import shutil\n",
        "import os\n",
        "\n",
        "spark = SparkSession.builder.appName('Test streaming').getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "\n",
        "fake = Faker()\n",
        "messages = [fake.uuid4() for _ in range(50)]\n",
        "\n",
        "def enrich_data(df, messages=messages):\n",
        "    fake = Faker()\n",
        "    new_columns = {\n",
        "        'event_type': F.lit(fake.random_element(elements=('OPEN', 'RECEIVED', 'SENT', 'CREATED', 'CLICKED', '', 'NONE'))),\n",
        "        'message_id': F.lit(fake.random_element(elements=messages)),\n",
        "        'channel': F.lit(fake.random_element(elements=('CHAT', 'EMAIL', 'SMS', 'PUSH', 'OTHER'))),\n",
        "        'country_id': F.lit(fake.random_int(min=2000, max=2015)),\n",
        "        'user_id': F.lit(fake.random_int(min=1000, max=1050)),\n",
        "    }\n",
        "    df = df.withColumns(new_columns)\n",
        "    return df\n",
        "\n",
        "def insert_messages(df: DataFrame, batch_id):\n",
        "    enrich = enrich_data(df)\n",
        "    enrich = enrich.withColumn(\"value\", F.col(\"value\").cast(\"string\"))\n",
        "    enrich.write.mode(\"append\").format(\"parquet\").save(\"/content/lake/bronze/messages/data\")\n",
        "\n",
        "# Delete existing data - For first execution there's no data to delete (as directory is different from the other Producer)\n",
        "bronze_path = \"/content/lake/bronze/messages/data\"\n",
        "if os.path.exists(bronze_path):\n",
        "    shutil.rmtree(bronze_path)\n",
        "\n",
        "# Read stream\n",
        "df_stream = spark.readStream.format(\"rate\").option(\"rowsPerSecond\", 1).load()\n",
        "\n",
        "# Write stream\n",
        "query = (df_stream.writeStream\n",
        "    .outputMode('append')\n",
        "    .trigger(processingTime='1 seconds')\n",
        "    .option(\"checkpointLocation\", \"/content/lake/bronze/messages/checkpoint\")\n",
        "    .foreachBatch(insert_messages)\n",
        "    .start()\n",
        ")\n",
        "\n",
        "# Run for 1 minute\n",
        "query.awaitTermination(60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XWYXoeHgco1",
        "outputId": "5911b47d-d801-4084-83a2-bbef24a3b324",
        "collapsed": true
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query.stop()"
      ],
      "metadata": {
        "id": "3Yxwu_tJjUIK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check New Producer results\n",
        "df = spark.read.format(\"parquet\").load(\"/content/lake/bronze/messages/data\")\n",
        "print(\"Total messages in bronze layer:\", df.count())\n",
        "\n",
        "# Check dataframe with a sample of results (with ~ [valid messages], without ~ [corrupted messages])\n",
        "df.filter(~((F.col(\"event_type\").isNull()) | (F.col(\"event_type\") == \"\") | (F.col(\"event_type\") == \"NONE\"))).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T5uLnxkNjF3w",
        "outputId": "8d441146-f3b3-4f3f-81f2-cfa53b8f798c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total messages in bronze layer: 74\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|           timestamp|value|event_type|          message_id|channel|country_id|user_id|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "|2024-12-16 12:19:...|   71|  RECEIVED|26ea780c-b1e3-4fe...|  EMAIL|      2003|   1007|\n",
            "|2024-12-16 12:19:...|   67|  RECEIVED|ab88e064-c736-42a...|  EMAIL|      2003|   1035|\n",
            "|2024-12-16 12:19:...|   34|  RECEIVED|e1321dba-d2b5-429...|   PUSH|      2006|   1024|\n",
            "|2024-12-16 12:19:...|   49|   CREATED|f1426824-a3e9-4f0...|  EMAIL|      2014|   1032|\n",
            "|2024-12-16 12:19:...|   53|  RECEIVED|3eecdacf-c88e-48f...|   PUSH|      2011|   1037|\n",
            "|2024-12-16 12:18:...|   17|   CREATED|3eecdacf-c88e-48f...|  EMAIL|      2003|   1012|\n",
            "|2024-12-16 12:18:...|   24|  RECEIVED|b128f5ae-814f-48b...|   CHAT|      2001|   1035|\n",
            "|2024-12-16 12:18:...|   20|  RECEIVED|d74e7b99-edb0-449...|   CHAT|      2012|   1028|\n",
            "|2024-12-16 12:18:...|   16|   CLICKED|d9b5bc92-1e04-4d2...|  EMAIL|      2014|   1006|\n",
            "|2024-12-16 12:18:...|   25|   CREATED|55937fe1-4639-4ca...|  EMAIL|      2010|   1005|\n",
            "|2024-12-16 12:19:...|   57|  RECEIVED|0317ee13-163b-4e4...|   PUSH|      2005|   1012|\n",
            "|2024-12-16 12:19:...|   33|   CREATED|619aa72f-c53f-4c2...|  OTHER|      2002|   1019|\n",
            "|2024-12-16 12:18:...|    6|  RECEIVED|ca64ad22-d390-4c4...|  OTHER|      2012|   1022|\n",
            "|2024-12-16 12:18:...|   19|   CLICKED|609bef70-52e9-46b...|  OTHER|      2009|   1036|\n",
            "|2024-12-16 12:18:...|   15|   CLICKED|c4ba6be4-a3e7-441...|  OTHER|      2001|   1003|\n",
            "|2024-12-16 12:18:...|   23|   CREATED|7c2ad6fa-9565-4a9...|  OTHER|      2007|   1033|\n",
            "|2024-12-16 12:19:...|   69|   CREATED|ceea93bc-21b8-417...|  OTHER|      2006|   1050|\n",
            "|2024-12-16 12:19:...|   36|   CLICKED|7c2ad6fa-9565-4a9...|  OTHER|      2007|   1006|\n",
            "|2024-12-16 12:19:...|   68|   CLICKED|827bbeb1-6d02-4fe...|  OTHER|      2008|   1043|\n",
            "|2024-12-16 12:19:...|   61|   CREATED|1f900c01-3eee-445...|   PUSH|      2004|   1012|\n",
            "+--------------------+-----+----------+--------------------+-------+----------+-------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvPj9hVpzNf"
      },
      "source": [
        "# Streaming Messages x Messages Corrupted"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Aux cell to delete silver folder for testing purpose\n",
        "silver_path = \"/content/lake/silver\"\n",
        "if os.path.exists(silver_path):\n",
        "    shutil.rmtree(silver_path)"
      ],
      "metadata": {
        "id": "aJhzZ1dZrLif"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, when, date_format, expr\n",
        "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType\n",
        "\n",
        "spark = SparkSession.builder.appName(\"MessageProcessing\").getOrCreate()\n",
        "\n",
        "# Define messages schema\n",
        "message_schema = StructType([\n",
        "    StructField(\"timestamp\", TimestampType()),\n",
        "    StructField(\"value\", StringType()),\n",
        "    StructField(\"event_type\", StringType()),\n",
        "    StructField(\"message_id\", StringType()),\n",
        "    StructField(\"channel\", StringType()),\n",
        "    StructField(\"country_id\", IntegerType()),\n",
        "    StructField(\"user_id\", IntegerType())\n",
        "])\n",
        "\n",
        "# Added countries data to this cell for join optimization by caching countries dataset\n",
        "countries = [\n",
        "    {\"country_id\": 2000, \"country\": \"Brazil\"},\n",
        "    {\"country_id\": 2001, \"country\": \"Portugal\"},\n",
        "    {\"country_id\": 2002, \"country\": \"Spain\"},\n",
        "    {\"country_id\": 2003, \"country\": \"Germany\"},\n",
        "    {\"country_id\": 2004, \"country\": \"France\"},\n",
        "    {\"country_id\": 2005, \"country\": \"Italy\"},\n",
        "    {\"country_id\": 2006, \"country\": \"United Kingdom\"},\n",
        "    {\"country_id\": 2007, \"country\": \"United States\"},\n",
        "    {\"country_id\": 2008, \"country\": \"Canada\"},\n",
        "    {\"country_id\": 2009, \"country\": \"Australia\"},\n",
        "    {\"country_id\": 2010, \"country\": \"Japan\"},\n",
        "    {\"country_id\": 2011, \"country\": \"China\"},\n",
        "    {\"country_id\": 2012, \"country\": \"India\"},\n",
        "    {\"country_id\": 2013, \"country\": \"South Korea\"},\n",
        "    {\"country_id\": 2014, \"country\": \"Russia\"},\n",
        "    {\"country_id\": 2015, \"country\": \"Argentina\"}\n",
        "]\n",
        "\n",
        "countries = spark.createDataFrame(countries).cache()\n",
        "\n",
        "\n",
        "def process_messages(df, batch_id):\n",
        "      # Join with countries\n",
        "      df_with_country = df.join(countries, \"country_id\", \"left\")\n",
        "\n",
        "      # Add date column\n",
        "      df_with_date = df_with_country.withColumn(\"date\", date_format(col(\"timestamp\"), \"yyyy-MM-dd\"))\n",
        "\n",
        "      # Split into corrupted and non-corrupted with corresponding logic\n",
        "      corrupted = df_with_date.filter((col(\"event_type\").isNull()) | (col(\"event_type\") == \"\") | (col(\"event_type\") == \"NONE\"))\n",
        "      valid = df_with_date.filter((col(\"event_type\").isNotNull()) & (col(\"event_type\") != \"\") & (col(\"event_type\") != \"NONE\"))\n",
        "\n",
        "      # Write corrupted data\n",
        "      corrupted.write \\\n",
        "          .partitionBy(\"date\") \\\n",
        "          .mode(\"append\") \\\n",
        "          .parquet(\"/content/lake/silver/messages_corrupted/data\")\n",
        "\n",
        "      # Write non-corrupted data\n",
        "      valid.write \\\n",
        "          .partitionBy(\"date\") \\\n",
        "          .mode(\"append\") \\\n",
        "          .parquet(\"/content/lake/silver/messages/data\")\n",
        "\n",
        "\n",
        "\n",
        "# Read from bronze layer\n",
        "df_stream = spark.readStream \\\n",
        "    .schema(message_schema) \\\n",
        "    .parquet(\"/content/lake/bronze/messages/data/*\")\n",
        "\n",
        "\n",
        "# Process stream\n",
        "query = df_stream.writeStream \\\n",
        "    .foreachBatch(process_messages) \\\n",
        "    .option(\"checkpointLocation\", \"/content/lake/silver/checkpoint\") \\\n",
        "    .trigger(processingTime='5 seconds') \\\n",
        "    .start()\n",
        "\n",
        "\n",
        "# Run for 20 seconds\n",
        "query.awaitTermination(20)\n",
        "query.stop()"
      ],
      "metadata": {
        "id": "4HlWxkLxDFsA"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLK9jpjCu3xE"
      },
      "source": [
        "## Checking data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "nk8seEvbmvcU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d6e6720-3811-4d32-bccf-41ea3d1a5045"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total messages in bronze layer: 74\n",
            "Total corrupted messages in silver layer: 17\n",
            "Total non-corrupted messages in silver layer: 57\n",
            "Total messages in silver layer: 74\n",
            "Assertion passed: Message counts match\n"
          ]
        }
      ],
      "source": [
        "# Check results\n",
        "bronze_count = spark.read.schema(message_schema).parquet(\"/content/lake/bronze/messages/data\").count()\n",
        "silver_corrupted_count = spark.read.parquet(\"/content/lake/silver/messages_corrupted/data\").count()\n",
        "silver_valid_count = spark.read.parquet(\"/content/lake/silver/messages/data\").count()\n",
        "\n",
        "print(f\"Total messages in bronze layer: {bronze_count}\")\n",
        "print(f\"Total corrupted messages in silver layer: {silver_corrupted_count}\")\n",
        "print(f\"Total non-corrupted messages in silver layer: {silver_valid_count}\")\n",
        "print(f\"Total messages in silver layer: {silver_corrupted_count + silver_valid_count}\")\n",
        "\n",
        "# Assertion to automaticaly evaluate the integraty of the number of records between bronze and silver layer\n",
        "try:\n",
        "    assert bronze_count == silver_corrupted_count + silver_valid_count, \"Mismatch in message counts\"\n",
        "    print(\"Assertion passed: Message counts match\")\n",
        "except AssertionError as e:\n",
        "    print(f\"Assertion failed: {str(e)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfxIlBISSvRP"
      },
      "source": [
        "# Challenge 2\n",
        "\n",
        "- Run business report\n",
        "- But first, there is a bug in the system which is causing some duplicated messages, we need to exclude these lines from the report\n",
        "\n",
        "- removing duplicates logic:\n",
        "  - Identify possible duplicates on message_id, event_type and channel\n",
        "  - in case of duplicates, consider only the first message (occurrence by timestamp)\n",
        "  - Ex:\n",
        "    In table below, the correct message to consider is the second line\n",
        "\n",
        "```\n",
        "    message_id | channel | event_type | timestamp\n",
        "    123        | CHAT    | CREATED    | 10:10:01\n",
        "    123        | CHAT    | CREATED    | 07:56:45 (first occurrence)\n",
        "    123        | CHAT    | CREATED    | 08:13:33\n",
        "```\n",
        "\n",
        "- After cleaning the data we're able to create the busines report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R3J9XyOHhqvU"
      },
      "outputs": [],
      "source": [
        "# dedup data\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "df = spark.read.format(\"parquet\").load(\"/content/lake/silver/messages\")\n",
        "dedup = df.withColumn(\"row_number\", F.row_number().over(Window.partitionBy(\"message_id\", \"event_type\", \"channel\").orderBy(\"timestamp\"))).filter(\"row_number = 1\").drop(\"row_number\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF9L9i25lk74"
      },
      "source": [
        "### Report 1\n",
        "  - Aggregate data by date, event_type and channel\n",
        "  - Count number of messages\n",
        "  - pivot event_type from rows into columns\n",
        "  - schema expected:\n",
        "  \n",
        "```\n",
        "|      date|channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|\n",
        "+----------+-------+-------+-------+----+--------+----+\n",
        "|2024-12-03|    SMS|      4|      4|   1|       1|   5|\n",
        "|2024-12-03|   CHAT|      3|      7|   5|       8|   4|\n",
        "|2024-12-03|   PUSH|   NULL|      3|   4|       3|   4|\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "UPHSMSXnTKgu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a880b6f-f638-44c4-ab7d-5e98e9685329"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- date: date (nullable = true)\n",
            " |-- channel: string (nullable = true)\n",
            " |-- CLICKED: long (nullable = true)\n",
            " |-- CREATED: long (nullable = true)\n",
            " |-- OPEN: long (nullable = true)\n",
            " |-- RECEIVED: long (nullable = true)\n",
            " |-- SENT: long (nullable = true)\n",
            " |-- total: long (nullable = false)\n",
            "\n",
            "+----------+-------+-------+-------+----+--------+----+-----+\n",
            "|date      |channel|CLICKED|CREATED|OPEN|RECEIVED|SENT|total|\n",
            "+----------+-------+-------+-------+----+--------+----+-----+\n",
            "|2024-12-16|CHAT   |1      |2      |4   |2       |2   |11   |\n",
            "|2024-12-16|EMAIL  |2      |3      |1   |2       |2   |10   |\n",
            "|2024-12-16|OTHER  |4      |3      |3   |1       |2   |13   |\n",
            "|2024-12-16|PUSH   |1      |5      |2   |4       |2   |14   |\n",
            "|2024-12-16|SMS    |NULL   |5      |1   |2       |1   |9    |\n",
            "+----------+-------+-------+-------+----+--------+----+-----+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 1\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Leitura dos dados deduplicados\n",
        "dedup = spark.read.parquet(\"/content/lake/silver/messages/data\")\n",
        "\n",
        "# Criação do Relatório 1\n",
        "report1 = (dedup\n",
        "    .withColumn(\"date\", F.to_date(\"timestamp\"))  # Extrair a data da coluna timestamp\n",
        "    .groupBy(\"date\", \"channel\")\n",
        "    .pivot(\"event_type\")\n",
        "    .agg(F.count(\"*\").alias(\"count\"))\n",
        "    .orderBy(\"date\", \"channel\")\n",
        ")\n",
        "\n",
        "# Adicionar uma coluna de total\n",
        "report1 = report1.withColumn(\"total\", sum(F.coalesce(report1[col], F.lit(0)) for col in report1.columns if col not in [\"date\", \"channel\"]))\n",
        "\n",
        "\n",
        "# Mostrar o esquema do relatório\n",
        "report1.printSchema()\n",
        "\n",
        "# Mostrar as primeiras linhas do relatório\n",
        "report1.show(truncate=False)\n",
        "\n",
        "# Salvar o relatório em formato parquet\n",
        "report1.write.mode(\"overwrite\").parquet(\"/content/lake/gold/report1/data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bxwOawo2lwQH"
      },
      "source": [
        "## Report 2\n",
        "\n",
        "- Identify the most active users by channel (sorted by number of iterations)\n",
        "- schema expected:\n",
        "\n",
        "```\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
        "+-------+----------+----+-----+-----+----+---+\n",
        "|   1022|         5|   2|    0|    1|   0|  2|\n",
        "|   1004|         4|   1|    1|    1|   1|  0|\n",
        "|   1013|         4|   0|    0|    2|   1|  1|\n",
        "|   1020|         4|   2|    0|    1|   1|  0|\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rsS7bkAJmWsW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7a7a5e-5937-46c3-d559-fc4cf54e1cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- user_id: integer (nullable = true)\n",
            " |-- iterations: long (nullable = true)\n",
            " |-- CHAT: long (nullable = true)\n",
            " |-- EMAIL: long (nullable = true)\n",
            " |-- OTHER: long (nullable = true)\n",
            " |-- PUSH: long (nullable = true)\n",
            " |-- SMS: long (nullable = true)\n",
            "\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|user_id|iterations|CHAT|EMAIL|OTHER|PUSH|SMS|\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "|1005   |1         |0   |1    |0    |0   |0  |\n",
            "|1031   |1         |0   |0    |0    |0   |1  |\n",
            "|1030   |2         |0   |0    |1    |1   |0  |\n",
            "|1019   |2         |1   |0    |1    |0   |0  |\n",
            "|1008   |1         |0   |0    |0    |1   |0  |\n",
            "|1047   |1         |0   |0    |0    |0   |1  |\n",
            "|1021   |1         |0   |0    |1    |0   |0  |\n",
            "|1026   |2         |0   |0    |0    |1   |1  |\n",
            "|1028   |3         |1   |1    |0    |1   |0  |\n",
            "|1032   |2         |1   |1    |0    |0   |0  |\n",
            "|1010   |1         |0   |1    |0    |0   |0  |\n",
            "|1050   |1         |0   |0    |1    |0   |0  |\n",
            "|1035   |2         |1   |1    |0    |0   |0  |\n",
            "|1045   |1         |0   |0    |0    |1   |0  |\n",
            "|1037   |2         |0   |0    |0    |1   |1  |\n",
            "|1036   |2         |0   |1    |1    |0   |0  |\n",
            "|1022   |4         |2   |0    |2    |0   |0  |\n",
            "|1015   |2         |0   |1    |0    |1   |0  |\n",
            "|1041   |1         |1   |0    |0    |0   |0  |\n",
            "|1001   |1         |0   |0    |0    |0   |1  |\n",
            "+-------+----------+----+-----+-----+----+---+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# report 2\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Leitura dos dados deduplicados\n",
        "dedup = spark.read.parquet(\"/content/lake/silver/messages/data\")\n",
        "\n",
        "# Criação do Relatório 2\n",
        "report2 = (dedup\n",
        "    .groupBy(\"user_id\", \"channel\")\n",
        "    .agg(F.count(\"*\").alias(\"interactions\"))\n",
        "    .groupBy(\"user_id\")\n",
        "    .pivot(\"channel\")\n",
        "    .agg(F.first(\"interactions\"))\n",
        "    .fillna(0)  # Replaces NULL values for 0\n",
        ")\n",
        "\n",
        "# Adicionar a coluna 'iterations' com o total de interações por usuário\n",
        "report2 = report2.withColumn(\"iterations\", sum(report2[col] for col in report2.columns if col != \"user_id\"))\n",
        "\n",
        "# Reordenar as colunas para que 'iterations' venha logo após 'user_id'\n",
        "columns_order = [\"user_id\", \"iterations\"] + [col for col in report2.columns if col not in [\"user_id\", \"iterations\"]]\n",
        "report2 = report2.select(columns_order)\n",
        "\n",
        "# Mostrar o esquema do relatório\n",
        "report2.printSchema()\n",
        "\n",
        "# Mostrar as primeiras linhas do relatório\n",
        "report2.show(truncate=False)\n",
        "\n",
        "# Salvar o relatório em formato parquet\n",
        "report2.write.mode(\"overwrite\").parquet(\"/content/lake/gold/report2/data\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9_kzDbDwDOS"
      },
      "source": [
        "# Challenge 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef0RjFTxwE5y"
      },
      "outputs": [],
      "source": [
        "# Theoretical question:\n",
        "\n",
        "# A new usecase requires the message data to be aggregate in near real time\n",
        "# They want to build a dashboard embedded in the platform website to analyze message data in low latency (few minutes)\n",
        "# This application will access directly the data aggregated by streaming process\n",
        "\n",
        "# Q1:\n",
        "- What would be your suggestion to achieve that using Spark Structure Streaming?\n",
        "Or would you choose a different data processing tool?\n",
        "\n",
        "- Which storage would you use and why? (database?, data lake?, kafka?)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To achieve **near real-time** aggregation of message data with low latency, using Spark Structured Streaming, it's an excellent choice for this use case due to the following reasons:\n",
        "\n",
        "*   Supports real-time event processing with exactly-once guarantees;\n",
        "*   Allows for event-time based window aggregations and use of watermarks to handle late data.´;\n",
        "*   Easily integrates with various data sources and sinks;\n",
        "*   Offers good scalability and fault tolerance.\n",
        "\n",
        "Suggested implementation:\n",
        "\n",
        "\n",
        "*   Use event-time based window aggregations (e.g., 1-minute windows);\n",
        "*   Set a short trigger interval (e.g., 30 seconds) for frequent updates;\n",
        "*   Use the \"append\" output mode to write only new aggregated results.\n",
        "\n",
        "\n",
        "\n",
        "For storage, I would recommend using a database optimized for real-time analytics, such as Apache Cassandra or Amazon DynamoDB. The reasons for this choice are:\n",
        "They offer low-latency reads and writes, essential for serving a real-time dashboard.\n",
        "Support high transaction rates, suitable for continuous ingestion of aggregated data.\n",
        "Allow for fast queries to feed the dashboard embedded in the platform website.\n",
        "Provide good horizontal scalability to handle growing data.\n",
        "\n",
        "\n",
        "Alternative: If ultra-low latency (sub-second) is critical, consider using Apache Kafka as an intermediate storage layer. This would allow consumers (like the dashboard) to subscribe directly to Kafka topics for real-time updates."
      ],
      "metadata": {
        "id": "7ZUXSU0QoEV7"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}